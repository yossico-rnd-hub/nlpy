{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from math import log\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, decaying, compounding\n",
    "\n",
    "use_titles = True\n",
    "do_clean_text = True\n",
    "model = 'en_core_web_sm'\n",
    "#model = 'en_core_web_lg'\n",
    "\n",
    "fp_texts = []\n",
    "fn_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    texts = []\n",
    "    true_labels = []\n",
    "    with path.open('r') as file_:\n",
    "        for row in csv.DictReader(file_, delimiter=','):\n",
    "            text = row['title'] if use_titles else row['text']\n",
    "            labels = row['labels'] # could be multiple labels per row\n",
    "            for label in labels.split(';'):\n",
    "                texts.append(text)\n",
    "                true_labels.append(label.strip())\n",
    "    return texts, true_labels\n",
    "\n",
    "\n",
    "def format_data_for_spacy(texts, labels, all_labels):\n",
    "    ys = []\n",
    "    for true_label in labels:\n",
    "        cats = {wrong_label: 0.0 for wrong_label in all_labels}\n",
    "        cats[true_label] = 1.0\n",
    "        ys.append({'cats': cats})\n",
    "    return list(zip(texts, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import groupby\n",
    "regex = re.compile(r\"n[ar]\\d+[a-z]*\") # e.g: na18020,nr18030ml\n",
    "\n",
    "def clean_text(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # merge entities\n",
    "    #for span in doc.ents:\n",
    "    #    span.merge()\n",
    "    \n",
    "    # normalize & filter tokens\n",
    "    words = []\n",
    "    for t in doc:\n",
    "        w = normalize_word(t)\n",
    "        if (None != w):\n",
    "            words.append(w)\n",
    "    \n",
    "    # remove duplicated consecutive terms (e.g: DATE DATE... -> DATE)\n",
    "    words = [x[0] for x in groupby(words)]\n",
    "    \n",
    "    # to string\n",
    "    return ' '.join(words)\n",
    "\n",
    "def normalize_word(t):\n",
    "    if (t.ent_type_ in ('DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL')):\n",
    "        return t.ent_type_\n",
    "    if t.like_num:\n",
    "        return 'LIKE_NUM'\n",
    "    if t.like_email:\n",
    "        return 'LIKE_EMAIL'\n",
    "    if t.like_url:\n",
    "        return 'LIKE_URL'\n",
    "    if t.is_punct:\n",
    "        return None\n",
    "#     if t.is_stop:\n",
    "#         return None\n",
    "#     if len(t.lemma_) < 3:\n",
    "#         return None\n",
    "    if regex.match(t.lemma_):\n",
    "        return None\n",
    "        \n",
    "    return t.lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nlp, texts, labels, split=0.8, n_iter=10):\n",
    "    \n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add labels\n",
    "    label_set = set(labels)\n",
    "    for label in label_set:\n",
    "        textcat.add_label(label)\n",
    "    \n",
    "    # split train/eval\n",
    "    data = format_data_for_spacy(texts, labels, label_set)\n",
    "    random.shuffle(data)\n",
    "    split = int(len(data) * split)\n",
    "    train_data = data[:split]\n",
    "    eval_data = data[split:]\n",
    "    dev_texts = []\n",
    "    dev_cats = []\n",
    "    for index, (t, c) in enumerate(eval_data):\n",
    "        dev_texts.append(t)\n",
    "        dev_cats.append(c)\n",
    "\n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(len(texts), len(train_data), len(eval_data)))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "#             batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "#             for batch in batches:\n",
    "            for batch in minibatch(train_data, size=128):\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, losses=losses, drop=0.3)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]['cats']\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "                fp_texts.append((label, texts[i]))\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "                fn_texts.append((label, texts[i]))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_dir = '../../../data/lapd.labeled'):\n",
    "    \n",
    "    print(\"loading model '{}'...\".format(model))\n",
    "    nlp = spacy.load(model)\n",
    "    print('model loaded.')\n",
    "\n",
    "    print()\n",
    "    print(\"loading data from '{}'...\".format(data_dir))\n",
    "    data_dir = Path(data_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for year in ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2018']:\n",
    "    #for year in ['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2017', '2018']:\n",
    "        t, l = load_data(data_dir / 'lapd_news_{}.csv'.format(year))\n",
    "        texts = texts + t\n",
    "        labels = labels + l\n",
    "    \n",
    "    if do_clean_text:\n",
    "        print()\n",
    "        print('cleaning texts...')\n",
    "        with nlp.disable_pipes('parser'):\n",
    "            clean_texts = [clean_text(nlp, text) for text in texts]\n",
    "            print('clean texts completed.')\n",
    "    else:\n",
    "        clean_texts = texts\n",
    "        \n",
    "#     print()\n",
    "#     for i in range(5):\n",
    "#         print('label:\\t', labels[i])\n",
    "#         print('text:\\t', texts[i])\n",
    "#         if do_clean_text:\n",
    "#             print('clean:\\t', clean_texts[i])\n",
    "#         print()\n",
    "\n",
    "    print()\n",
    "    train(nlp, clean_texts, labels)\n",
    "\n",
    "    print()\n",
    "    print('Done.')\n",
    "    \n",
    "    # FP TEXTS\n",
    "    fp_csv = '../../../data/fp.csv'\n",
    "    with open(fp_csv, 'w') as _file:\n",
    "        writer = csv.writer(_file)\n",
    "        writer.writerow(['label', 'text'])\n",
    "        for t in fp_texts:\n",
    "            writer.writerow([t[0], t[1]])\n",
    "    \n",
    "    # FN TEXTS\n",
    "    fn_csv = '../../../data/fn.csv'\n",
    "    with open(fn_csv, 'w') as _file:\n",
    "        writer = csv.writer(_file)\n",
    "        writer.writerow(['label', 'text'])\n",
    "        for t in fn_texts:\n",
    "            writer.writerow([t[0], t[1]])\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
