{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gensim LDA/LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "prepare the dataset we’ll be working with.\n",
    "<br>(array of texts of news titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 13956 documens\n",
      "(top 5)\n",
      "Two Suspects Arrested in Home Invasion Robbery    NR17032ma \n",
      "Suspect Arrested for Chinatown Murders    NR17033ml \n",
      "Hit and Run Collision Leaves Pedestrian Dead    NR17033ne \n",
      "Fatal Stabbing of a 27-year-old Man   NR17035im \n",
      "Press Conference   NA17014ma\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "dirname = '../../../out'\n",
    "if os.path.exists(dirname):   \n",
    "    for filename in os.listdir(dirname):\n",
    "        filename = os.path.join(dirname, filename)\n",
    "        with open(filename, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                data.append(row[0])\n",
    "\n",
    "# print first 5 titles\n",
    "NUM_DOCUMENTS = len(data)\n",
    "print('data:', NUM_DOCUMENTS, 'documens')\n",
    "print('(top 5)')\n",
    "print(' \\n'.join(data[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data for lad/lsi (tokenize and clean the data: stopwords, etc.)\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    \n",
    "    # simple_preprocess: lowercases, tokenizes, de-accents (optional). \n",
    "    # deacc=True removes punctuations.\n",
    "    for token in simple_preprocess(text, deacc=True, min_len=2, max_len=15):\n",
    "        if (token not in STOPWORDS\n",
    "            and len(token) > 3\n",
    "            and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', token)):\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "# preprocess data\n",
    "processed_docs = []\n",
    "for text in data:\n",
    "    processed_docs.append(preprocess(text))\n",
    "\n",
    "# print first 5 processed titles\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA/LSI using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Gensim LDA and LSI models using BOW corpus.\n",
    "\n",
    "from gensim import models, corpora\n",
    "\n",
    "NUM_TOPICS = 30\n",
    "\n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Gensim filter_extremes\n",
    "# dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Transform the collection of texts to a numerical form\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_docs]\n",
    " \n",
    "# Have a look at how the 10th document looks like: [(word_id, count), ...]\n",
    "# e.g: [(12, 3), (14, 1), (21, 1), (25, 5), (30, 2), (31, 5), (33, 1), (42, 1), (43, 2),  ...\n",
    "doc_id = 10\n",
    "print(processed_docs[doc_id])\n",
    "print(bow_corpus[doc_id])\n",
    "\n",
    "# Build the LDA model (Latent Dirichlet Allocation)\n",
    "lda_model_bow = gensim.models.LdaMulticore(corpus=bow_corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "# Build the LSI model (Latent Semantic Analysis or Latent Semantic Indexing)\n",
    "lsi_model_bow = models.LsiModel(corpus=bow_corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, print the first 5 most representative topics.\n",
    "\n",
    "print(\"LDA Model (bow):\")\n",
    "print(\"=\" * 20)\n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 5 most representative topics\n",
    "    print(\"Topic %s:\" % idx, lda_model_bow.print_topic(idx, 5)) \n",
    "\n",
    "print()\n",
    "\n",
    "print(\"LSI Model (bow):\")\n",
    "print(\"=\" * 20)\n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 5 most representative topics\n",
    "    print(\"Topic %s:\" % idx, lsi_model_bow.print_topic(idx, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let’s now put the models to work and transform unseen documents to their topic distribution:\n",
    "\n",
    "texts = [\n",
    "    \"A men found killed in the park.\", \n",
    "    \"A woman was raped in the park.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print()\n",
    "    print(text)\n",
    "    \n",
    "    bow = dictionary.doc2bow(preprocess(text))\n",
    "\n",
    "    # print(lda_model[bow])\n",
    "    # [(0, 0.020005183), (1, 0.020005869), (2, 0.02000626), (3, 0.020005472), (4, 0.020009108), (5, 0.020005926), (6, 0.81994385), (7, 0.020006068), (8, 0.020006327), (9, 0.020005994)]\n",
    "    # print(max(lda_model[bow], key=lambda item:item[1]))\n",
    "    print('lda:', sorted(lda_model_bow[bow], key=lambda item:item[1], reverse=True)[:3])\n",
    "\n",
    "    # print(lsi_model[bow])\n",
    "    # [(0, 0.091615426138426506), (1, -0.0085557463300508351), (2, 0.016744863677828108), (3, 0.040508186718598529), (4, 0.014201267714185898), (5, -0.012208538275305329), (6, 0.031254053085582149), (7, 0.017529584659403553), (8, 0.056957633371540077), (9, 0.025989149894888153)]\n",
    "    print('lsi:', sorted(lsi_model_bow[bow], key=lambda item:item[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA/LSI using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Build the LDA model (Latent Dirichlet Allocation)\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus=tfidf_corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "# Build the LSI model (Latent Semantic Analysis or Latent Semantic Indexing)\n",
    "lsi_model_tfidf = models.LsiModel(corpus=tfidf_corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, print the first 5 most representative topics.\n",
    "\n",
    "print(\"LDA Model (tfidf):\")\n",
    "print(\"=\" * 20)\n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 5 most representative topics\n",
    "    print(\"Topic %s:\" % idx, lda_model_tfidf.print_topic(idx, 5)) \n",
    "\n",
    "print()\n",
    "\n",
    "print(\"LSI Model (tfidf):\")\n",
    "print(\"=\" * 20)\n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 5 most representative topics\n",
    "    print(\"Topic %s:\" % idx, lsi_model_tfidf.print_topic(idx, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk.stem\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "# vectorizer = StemmedCountVectorizer(\n",
    "#     analyzer=\"word\", \n",
    "#     min_df=5, max_df=0.9, \n",
    "#     stop_words='english', lowercase=True, \n",
    "#     token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                ngram_range = (1,2), \n",
    "                                min_df = 20,\n",
    "                                max_df = 1.0)\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    "\n",
    "# vectorizer = CountVectorizer(\n",
    "#     min_df=5, max_df=0.9, \n",
    "#     stop_words='english', lowercase=True, \n",
    "#     token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "# data_vectorized = vectorizer.fit_transform(data)\n",
    "\n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Non-Negative Matrix Factorization Model\n",
    "nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "nmf_Z = nmf_model.fit_transform(data_vectorized)\n",
    "print(nmf_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Latent Semantic Indexing Model\n",
    "lsi_model = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "lsi_Z = lsi_model.fit_transform(data_vectorized)\n",
    "print(lsi_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    " \n",
    "# Let's see how the first document in the corpus looks like in different topic spaces\n",
    "print(lda_Z[0])\n",
    "print(nmf_Z[0])\n",
    "print(lsi_Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=6):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print()\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print(\"LDA Model:\")\n",
    "print(\"=\" * 20)\n",
    "print_topics(lda_model, vectorizer)\n",
    " \n",
    "print()\n",
    "print(\"NMF Model:\")\n",
    "print(\"=\" * 20)\n",
    "print_topics(nmf_model, vectorizer)\n",
    "\n",
    "print()\n",
    "print(\"LSI Model:\")\n",
    "print(\"=\" * 20)\n",
    "print_topics(lsi_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming an unseen document\n",
    "texts = [\n",
    "    \"A men found killed in the park.\", \n",
    "    \"A woman was raped in the park.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print()\n",
    "    print(text)\n",
    "    x = nmf_model.transform(vectorizer.transform([text]))[0]\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting words and documents in 2D with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot documents in 2D\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "documents_2d = svd.fit_transform(data_vectorized)\n",
    " \n",
    "df = pd.DataFrame(columns=['x', 'y', 'document'])\n",
    "df['x'], df['y'], df['document'] = documents_2d[:,0], documents_2d[:,1], range(len(data))\n",
    " \n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"document\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "# plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display words in 2D we just need to transpose the vectorized data: \n",
    "# words_2d = svd.fit_transform(data_vectorized.T).\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "words_2d = svd.fit_transform(data_vectorized.T)\n",
    " \n",
    "df = pd.DataFrame(columns=['x', 'y', 'word'])\n",
    "df['x'], df['y'], df['word'] = words_2d[:,0], words_2d[:,1], vectorizer.get_feature_names()\n",
    " \n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "NUM_TOPICS = 40\n",
    "\n",
    "stem = False\n",
    "\n",
    "if stem:\n",
    "    vectorizer = StemmedCountVectorizer(\n",
    "        analyzer=\"word\", \n",
    "        min_df=5, max_df=0.9, \n",
    "        stop_words='english', lowercase=True, \n",
    "        token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "else:\n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#         min_df=5, max_df=0.9, \n",
    "#         stop_words='english', lowercase=True, \n",
    "#         token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "\n",
    "#     vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "    vectorizer = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                    stop_words = 'english',\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                    ngram_range = (1,2), \n",
    "                                    min_df = 20,\n",
    "                                    max_df = 1.0)\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    " \n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "# Transforming an unseen document\n",
    "texts = [\n",
    "    \"A men found killed in the park.\", \n",
    "    \"A woman was raped in the park.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print()\n",
    "    print(text)\n",
    "    x = lda_model.transform(vectorizer.transform([text]))[0]\n",
    "    print(x, x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
