{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "from math import log\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, decaying, compounding\n",
    "\n",
    "use_titles = False\n",
    "do_clean_text = False\n",
    "model = 'en_core_web_sm'\n",
    "\n",
    "n_iter=30\n",
    "drop=0.3\n",
    "score_treshold=0.6\n",
    "\n",
    "fp_texts = []\n",
    "fn_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    texts = []\n",
    "    true_labels = []\n",
    "    with path.open('r') as file_:\n",
    "        for row in csv.DictReader(file_, delimiter=','):\n",
    "            # text\n",
    "            text = row['title'] if use_titles else row['text']\n",
    "            texts.append(text)\n",
    "            # labels\n",
    "            raw_labels = row['labels'] # could be multiple labels per row (sparated by ';')\n",
    "            labels = list(map(lambda label: label.strip(), raw_labels.split('|')))\n",
    "            true_labels.append(labels)\n",
    "                \n",
    "    return texts, true_labels\n",
    "\n",
    "\n",
    "def format_data_for_spacy(texts, labels, all_labels):\n",
    "    ys = []    \n",
    "    for true_labels in labels:\n",
    "        cats = {wrong_label: 0.0 for wrong_label in all_labels}\n",
    "        for true_label in true_labels:\n",
    "            cats[true_label] = 1.0\n",
    "        ys.append({'cats': cats})\n",
    "    return list(zip(texts, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import groupby\n",
    "regex = re.compile(r\"n[ar]\\d+[a-z]*\") # e.g: na18020,nr18030ml\n",
    "\n",
    "def clean_text(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # merge entities\n",
    "    #for span in doc.ents:\n",
    "    #    span.merge()\n",
    "    \n",
    "    # normalize & filter tokens\n",
    "    words = []\n",
    "    for t in doc:\n",
    "        w = normalize_word(t)\n",
    "        if (None != w):\n",
    "            words.append(w)\n",
    "    \n",
    "    # remove duplicated consecutive terms (e.g: DATE DATE... -> DATE)\n",
    "    words = [x[0] for x in groupby(words)]\n",
    "    \n",
    "    # to string\n",
    "    return ' '.join(words)\n",
    "\n",
    "def normalize_word(t):\n",
    "    if (t.ent_type_ in ('DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL')):\n",
    "        return t.ent_type_\n",
    "    if t.like_num:\n",
    "        return 'LIKE_NUM'\n",
    "    if t.like_email:\n",
    "        return 'LIKE_EMAIL'\n",
    "    if t.like_url:\n",
    "        return 'LIKE_URL'\n",
    "    if t.is_punct:\n",
    "        return None\n",
    "#     if t.is_stop:\n",
    "#         return None\n",
    "#     if len(t.lemma_) < 3:\n",
    "#         return None\n",
    "    if regex.match(t.lemma_):\n",
    "        return None\n",
    "        \n",
    "    return t.lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nlp, texts, labels, split=0.8, n_iter=n_iter):\n",
    "    \n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add labels\n",
    "    labels2 = []\n",
    "    for labels_ in labels:\n",
    "        for label in labels_:\n",
    "            labels2.append(label)\n",
    "    \n",
    "    label_set = set(labels2)\n",
    "    for label in label_set:\n",
    "        textcat.add_label(label)\n",
    "    \n",
    "    # split train/eval\n",
    "    data = format_data_for_spacy(texts, labels, label_set)\n",
    "    random.shuffle(data)\n",
    "    split = int(len(data) * split)\n",
    "    train_data = data[:split]\n",
    "    eval_data = data[split:]\n",
    "    dev_texts = []\n",
    "    dev_cats = []\n",
    "    for index, (t, c) in enumerate(eval_data):\n",
    "        dev_texts.append(t)\n",
    "        dev_cats.append(c)\n",
    "\n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(len(texts), len(train_data), len(eval_data)))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            #for batch in minibatch(train_data, size=compounding(16., 128., 1.01)):\n",
    "            for batch in minibatch(train_data, size=128):\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, losses=losses, drop=drop)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats, i+1 == n_iter)\n",
    "\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats, do_fp_fn):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]['cats']\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= score_treshold and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= score_treshold and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "                if do_fp_fn:\n",
    "                    fp_texts.append((label, texts[i]))\n",
    "            elif score < score_treshold and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < score_treshold and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "                if do_fp_fn:\n",
    "                    fn_texts.append((label, texts[i]))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model 'en_core_web_sm'...\n",
      "\n",
      "loading data from '../../../data/lapd.labeled'...\n",
      "\n",
      "Using 4879 examples (3903 training, 976 evaluation)\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "53.666\t0.845\t0.433\t0.573\n",
      "18.561\t0.862\t0.430\t0.574\n",
      "16.091\t0.873\t0.462\t0.604\n",
      "14.715\t0.866\t0.479\t0.617\n",
      "14.119\t0.859\t0.509\t0.639\n",
      "12.518\t0.858\t0.518\t0.646\n",
      "11.197\t0.852\t0.546\t0.665\n",
      "10.585\t0.849\t0.558\t0.674\n",
      "10.097\t0.850\t0.567\t0.680\n",
      "10.021\t0.851\t0.570\t0.682\n",
      "9.685\t0.845\t0.567\t0.679\n",
      "9.655\t0.841\t0.575\t0.683\n",
      "9.561\t0.843\t0.569\t0.679\n",
      "9.571\t0.839\t0.573\t0.681\n",
      "9.782\t0.841\t0.564\t0.675\n",
      "10.129\t0.848\t0.562\t0.676\n",
      "10.139\t0.845\t0.567\t0.679\n",
      "11.125\t0.840\t0.566\t0.676\n",
      "10.239\t0.840\t0.569\t0.678\n",
      "9.237\t0.841\t0.575\t0.683\n",
      "8.586\t0.838\t0.578\t0.684\n",
      "7.950\t0.833\t0.579\t0.683\n",
      "7.712\t0.830\t0.582\t0.684\n",
      "7.474\t0.830\t0.587\t0.688\n",
      "7.528\t0.829\t0.597\t0.694\n",
      "7.680\t0.832\t0.599\t0.697\n",
      "7.274\t0.833\t0.599\t0.697\n",
      "7.172\t0.834\t0.603\t0.700\n",
      "7.089\t0.832\t0.602\t0.699\n",
      "7.128\t0.829\t0.606\t0.700\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def main(data_dir = '../../../data/lapd.labeled'):\n",
    "    \n",
    "    print(\"loading model '{}'...\".format(model))\n",
    "    nlp = spacy.load(model)\n",
    "\n",
    "    print()\n",
    "    print(\"loading data from '{}'...\".format(data_dir))\n",
    "    data_dir = Path(data_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    #for year in ['2018']:\n",
    "    for year in ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']:\n",
    "    #for year in ['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']:\n",
    "        csv_path = data_dir / 'lapd_news_{}.csv'.format(year)\n",
    "        if csv_path.exists():\n",
    "            t, l = load_data(csv_path)\n",
    "            texts = texts + t   # array concatination\n",
    "            labels = labels + l # array concatination\n",
    "    \n",
    "    if do_clean_text:\n",
    "        print()\n",
    "        print('cleaning texts...')\n",
    "        with nlp.disable_pipes('parser'):\n",
    "            clean_texts = [clean_text(nlp, text) for text in texts]\n",
    "            print('clean texts completed.')\n",
    "    else:\n",
    "        clean_texts = texts\n",
    "        \n",
    "#     print()\n",
    "#     for i in range(5):\n",
    "#         print('label:\\t', *labels[i])\n",
    "#         print('text:\\t', texts[i])\n",
    "#         if do_clean_text:\n",
    "#             print('clean:\\t', clean_texts[i])\n",
    "#         print()\n",
    "\n",
    "    print()\n",
    "    train(nlp, clean_texts, labels)\n",
    "\n",
    "    print()\n",
    "    print('Done.')\n",
    "    \n",
    "    # FP TEXTS\n",
    "    fp_csv = '../../../data/fp.csv'\n",
    "    with open(fp_csv, 'w') as _file:\n",
    "        writer = csv.writer(_file)\n",
    "        writer.writerow(['label', 'text'])\n",
    "        for t in set(fp_texts):\n",
    "            writer.writerow([t[0], t[1]])\n",
    "    \n",
    "    # FN TEXTS\n",
    "    fn_csv = '../../../data/fn.csv'\n",
    "    with open(fn_csv, 'w') as _file:\n",
    "        writer = csv.writer(_file)\n",
    "        writer.writerow(['label', 'text'])\n",
    "        for t in set(fn_texts):\n",
    "            writer.writerow([t[0], t[1]])\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
